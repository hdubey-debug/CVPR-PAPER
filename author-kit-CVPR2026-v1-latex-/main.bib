@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@misc{Authors14,
 author = {FirstName LastName},
 title = {The frobnicatable foo filter},
 note = {Face and Gesture submission ID 324. Supplied as supplemental material {\tt fg324.pdf}},
 year = 2014
}

@misc{Authors14b,
 author = {FirstName LastName},
 title = {Frobnication tutorial},
 note = {Supplied as supplemental material {\tt tr.pdf}},
 year = 2014
}

@article{Alpher02,
author = {FirstName Alpher},
title = {Frobnication},
journal = PAMI,
volume = 12,
number = 1,
pages = {234--778},
year = 2002
}

@article{Alpher03,
author = {FirstName Alpher and  FirstName Fotheringham-Smythe},
title = {Frobnication revisited},
journal = {Journal of Foo},
volume = 13,
number = 1,
pages = {234--778},
year = 2003
}

@article{Alpher04,
author = {FirstName Alpher and FirstName Fotheringham-Smythe and FirstName Gamow},
title = {Can a machine frobnicate?},
journal = {Journal of Foo},
volume = 14,
number = 1,
pages = {234--778},
year = 2004
}

@inproceedings{Alpher05,
author = {FirstName Alpher and FirstName Gamow},
title = {Can a computer frobnicate?},
booktitle = CVPR,
pages = {234--778},
year = 2005
}

%%%%%%%%% Video Language Models %%%%%%%%%

@misc{Yuan2025Tarsier2,
  author = {Liping Yuan and Jiawei Wang and Haomiao Sun and Yuchen Zhang and Yuan Lin},
  title = {Tarsier2: Advancing Large Vision-Language Models from Detailed Video Description to Comprehensive Video Understanding},
  year = 2025,
  eprint = {2501.07888},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV}
}

@inproceedings{Shen2025LongVU,
  author = {Xiaoqian Shen and Yunyang Xiong and Changsheng Zhao and Lemeng Wu and Jun Chen and Chenchen Zhu and Zechun Liu and Fanyi Xiao and Balakrishnan Varadarajan and Florian Bordes and Zhuang Liu and Hu Xu and Hyunwoo J. Kim and Bilge Soran and Raghuraman Krishnamoorthi and Mohamed Elhoseiny and Vikas Chandra},
  title = {{LongVU}: Spatiotemporal Adaptive Compression for Long Video-Language Understanding},
  booktitle = {Proc. Int. Conf. Mach. Learn.},
  year = 2025
}

@inproceedings{Shu2025VideoXL,
  author = {Yan Shu and Zheng Liu and Peitian Zhang and Minghao Qin and Junjie Zhou and Zhengyang Liang and Tiejun Huang and Bo Zhao},
  title = {Video-{XL}: Extra-Long Vision Language Model for Hour-Scale Video Understanding},
  booktitle = CVPR,
  pages = {26160--26169},
  year = 2025
}

@inproceedings{Chen2025LongVILA,
  author = {Yukang Chen and Fuzhao Xue and Dacheng Li and Qinghao Hu and Ligeng Zhu and Xiuyu Li and Yunhao Fang and Haotian Tang and Shang Yang and Zhijian Liu and Yihui He and Hongxu Yin and Pavlo Molchanov and Jan Kautz and Linxi Fan and Yuke Zhu and Yao Lu and Song Han},
  title = {{LongVILA}: Scaling Long-Context Visual Language Models for Long Videos},
  booktitle = ICLR,
  year = 2025
}

@inproceedings{Ataallah2024Goldfish,
  author = {Kirolos Ataallah and Xiaoqian Shen and Eslam Abdelrahman and Essam Sleiman and Mingchen Zhuge and Jian Ding and Deyao Zhu and J{\"u}rgen Schmidhuber and Mohamed Elhoseiny},
  title = {Goldfish: Vision-Language Understanding of Arbitrarily Long Videos},
  booktitle = ECCV,
  pages = {251--267},
  year = 2024
}

@inproceedings{Weng2024LongVLM,
  author = {Yuetian Weng and Mingfei Han and Haoyu He and Xiaojun Chang and Bohan Zhuang},
  title = {{LongVLM}: Efficient Long Video Understanding via Large Language Models},
  booktitle = ECCV,
  pages = {453--470},
  year = 2024
}

@inproceedings{He2024MALMM,
  author = {Bo He and Hengduo Li and Young Kyun Jang and Menglin Jia and Xuefei Cao and Ashish Shah and Abhinav Shrivastava and Ser{-}Nam Lim},
  title = {{MA-LMM}: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding},
  booktitle = CVPR,
  pages = {13504--13514},
  year = 2024
}

@misc{Cheng2024LongVideoLLaMA,
  author = {Zesen Cheng and Sicong Leng and Hang Zhang and Yifei Xin and Xin Li and Guanzheng Chen and Yongxin Zhu and Wenqi Zhang and Ziyang Luo and Deli Zhao and Lidong Bing},
  title = {Long-{LLaMA}: Extending {LLaMA} for Long Video Understanding with Temporal Attention},
  year = 2024,
  eprint = {2409.14405},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV}
}

@inproceedings{Chen2024VideoLLMonline,
  author = {Joya Chen and Zhaoyang Lv and Shiwei Wu and Qinghong Lin and Chenan Song and Difei Gao and Jia{-}Wei Liu and Ziteng Gao and Dongxing Mao and Mike Zheng Shou},
  title = {{VideoLLM-online}: Online Video Large Language Model for Streaming Video Understanding},
  booktitle = CVPR,
  pages = {18407--18418},
  year = 2024
}

@inproceedings{Ren2024TimeChat,
  author = {Shuhuai Ren and Linli Yao and Shicheng Li and Xu Sun and Lu Hou},
  title = {{TimeChat}: A Time-sensitive Multimodal Large Language Model for Long Video Understanding},
  booktitle = CVPR,
  pages = {14313--14323},
  year = 2024
}

@inproceedings{Qian2024VideoStreaming,
  author = {Rui Qian and Xiaoyi Dong and Pan Zhang and Yuhang Zang and Shuangrui Ding and Dahua Lin and Jiaqi Wang},
  title = {Streaming Long Video Understanding with Large Language Models},
  booktitle = NIPS,
  year = 2024
}

%%%%%%%%% Video Understanding Benchmarks %%%%%%%%%

@inproceedings{Chen2011MSR,
  author = {David L. Chen and William B. Dolan},
  title = {Collecting Highly Parallel Data for Paraphrase Evaluation},
  booktitle = {Proc. Annu. Meeting Assoc. Comput. Linguistics},
  year = 2011
}

@inproceedings{Zhou2018CrossTask,
  author = {Luowei Zhou and Chenliang Xu and Jason J. Corso},
  title = {Towards Automatic Learning of Procedures From Web Instructional Videos},
  booktitle = AAAI,
  pages = {7590--7598},
  year = 2018
}

@inproceedings{Wu2024LongVideoBench,
  author = {Haoning Wu and Dongxu Li and Bei Chen and Junnan Li},
  title = {{LongVideoBench}: A Benchmark for Long-context Interleaved Video-Language Understanding},
  booktitle = NIPS,
  year = 2024
}

@inproceedings{Ataallah2024InfiniBench,
  author = {Kirolos Ataallah and Chenhui Gou and Eslam Abdelrahman and Khushbu Pahwa and Jian Ding and Mohamed Elhoseiny},
  title = {{InfiniBench}: A Comprehensive Benchmark for Large Multimodal Models in Very Long Video Understanding},
  booktitle = ICLR,
  year = 2025
}

@misc{Du2024EventOriented,
  author = {Yifan Du and Kun Zhou and Yuqi Huo and Yifan Li and Wayne Xin Zhao and Haoyu Lu and Zijia Zhao and Bingning Wang and Weipeng Chen and Ji-Rong Wen},
  title = {Towards Event-oriented Long Video Understanding},
  year = 2024,
  eprint = {2406.14129},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV}
}

@inproceedings{Fang2024MMBenchVideo,
  author = {Xinyu Fang and Kangrui Mao and Haodong Duan and Xiangyu Zhao and Yining Li and Dahua Lin and Kai Chen},
  title = {{MMBench-Video}: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding},
  booktitle = NIPS,
  year = 2024
}

@inproceedings{Nagrani2025Neptune,
  author = {Arsha Nagrani and Mingda Zhang and Ramin Mehran and Rachel Hornung and Nitesh Bharadwaj Gundavarapu and Nilpa Jha and Austin Myers and Xingyi Zhou and Boqing Gong and Cordelia Schmid and Mikhail Sirotenko and Yukun Zhu and Tobias Weyand},
  title = {Neptune: The Long Orbit to Benchmarking Long Video Understanding},
  booktitle = ICLR,
  year = 2025
}

@misc{Wang2024LVBench,
  author = {Weihan Wang and Zehai He and Wenyi Hong and Yean Cheng and Xiaohan Zhang and Ji Qi and Xiaotao Gu and Shiyu Huang and Bin Xu and Yuxiao Dong and Ming Ding and Jie Tang},
  title = {{LVBench}: An Extreme Long Video Understanding Benchmark},
  year = 2024,
  eprint = {2406.08035},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV}
}

@inproceedings{Zhao2024VideoNIAH,
  author = {Zijia Zhao and Haoyu Lu and Yuqi Huo and Yifan Du and Tongtian Yue and Longteng Guo and Bingning Wang and Weipeng Chen and Jing Liu},
  title = {Needle in a Video Haystack: A Scalable Synthetic Framework for Benchmarking Video {MLLMs}},
  booktitle = ICLR,
  year = 2025
}

@inproceedings{Zhou2025MLVU,
  author = {Junjie Zhou and Yan Shu and Bo Zhao and Boya Wu and Shitao Xiao and Xi Yang and Yongping Xiong and Bo Zhang and Tiejun Huang and Zheng Liu},
  title = {{MLVU}: A Comprehensive Benchmark for Multi-Task Long Video Understanding},
  booktitle = CVPR,
  year = 2025
}

@inproceedings{Fu2025VideoMME,
  author = {Chaoyou Fu and Yuhan Dai and Yongdong Luo and Lei Li and Shuhuai Ren and Renrui Zhang and Zihan Wang and Chenyu Zhou and Yunhang Shen and Mengdan Zhang and Peixian Chen and Yanwei Li and Shaohui Lin and Sirui Zhao and Ke Li and Tong Xu and Xiawu Zheng and Enhong Chen and Caifeng Shan and Ran He and Xing Sun},
  title = {Video-{MME}: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal {LLMs} in Video Analysis},
  booktitle = CVPR,
  year = 2025
}

@inproceedings{Mangalam2023EgoSchema,
  author = {Karttikeya Mangalam and Raiymbek Akshulakov and Jitendra Malik},
  title = {{EgoSchema}: A Diagnostic Benchmark for Very Long-form Video Language Understanding},
  booktitle = NIPS,
  year = 2023
}

@inproceedings{Li2024MVBench,
  author = {Kunchang Li and Yali Wang and Yinan He and Yizhuo Li and Yi Wang and Yi Liu and Zun Wang and Limin Wang and Yu Qiao},
  title = {{MVBench}: A Comprehensive Multi-modal Video Understanding Benchmark},
  booktitle = CVPR,
  pages = {22195--22206},
  year = 2024
}

@inproceedings{Rohrbach2015MPII,
  author = {Anna Rohrbach and Marcus Rohrbach and Niket Tandon and Bernt Schiele},
  title = {A Dataset for Movie Description},
  booktitle = CVPR,
  year = 2015
}

@inproceedings{Frohmann2024SAT,
  author = {Markus Frohmann and Igor Sterner and Ivan Vuli{\'c} and Benjamin Minixhofer and Markus Schedl},
  title = {Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation},
  booktitle = {Proc. Conf. Empirical Methods Natural Lang. Process.},
  pages = {11908--11941},
  year = 2024
}

%%%%%%%%% Traditional Evaluation Metrics %%%%%%%%%

@inproceedings{Papineni2002BLEU,
  author = {Kishore Papineni and Salim Roukos and Todd Ward and Wei-Jing Zhu},
  title = {{BLEU}: A Method for Automatic Evaluation of Machine Translation},
  booktitle = {Proc. Annu. Meeting Assoc. Comput. Linguistics},
  pages = {311--318},
  year = 2002
}

@inproceedings{Lin2004ROUGE,
  author = {Chin-Yew Lin},
  title = {{ROUGE}: A Package for Automatic Evaluation of Summaries},
  booktitle = {Text Summarization Branches Out},
  pages = {74--81},
  year = 2004
}

@inproceedings{Banerjee2005METEOR,
  author = {Satanjeev Banerjee and Alon Lavie},
  title = {{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments},
  booktitle = {Proc. {ACL} Workshop Intrinsic Extrinsic Eval. Measures Mach. Translation Summarization},
  pages = {65--72},
  year = 2005
}

@inproceedings{Vedantam2015CIDEr,
  author = {Ramakrishna Vedantam and C. Lawrence Zitnick and Devi Parikh},
  title = {{CIDEr}: Consensus-Based Image Description Evaluation},
  booktitle = CVPR,
  pages = {4566--4575},
  year = 2015
}

@inproceedings{Anderson2016SPICE,
  author = {Peter Anderson and Basura Fernando and Mark Johnson and Stephen Gould},
  title = {{SPICE}: Semantic Propositional Image Caption Evaluation},
  booktitle = ECCV,
  pages = {382--398},
  year = 2016
}

%%%%%%%%% Embedding-Based Metrics %%%%%%%%%

@inproceedings{Zhang2020BERTScore,
  author = {Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
  title = {{BERTScore}: Evaluating Text Generation with {BERT}},
  booktitle = ICLR,
  year = 2020
}

@inproceedings{Zhao2019MoverScore,
  author = {Wei Zhao and Maxime Peyrard and Fei Liu and Yang Gao and Christian M. Meyer and Steffen Eger},
  title = {{MoverScore}: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance},
  booktitle = {Proc. Conf. Empirical Methods Natural Lang. Process.},
  pages = {563--578},
  year = 2019
}

@inproceedings{Reimers2019SBERT,
  author = {Nils Reimers and Iryna Gurevych},
  title = {Sentence-{BERT}: Sentence Embeddings using Siamese {BERT}-Networks},
  booktitle = {Proc. Conf. Empirical Methods Natural Lang. Process.},
  pages = {3982--3992},
  year = 2019
}

@misc{Lee2024NVEmbed,
  author = {Chankyu Lee and Rajarshi Roy and Mengyao Xu and Jonathan Raiman and Mohammad Shoeybi and Bryan Catanzaro and Wei Ping},
  title = {{NV-Embed}: Improved Techniques for Training {LLMs} as Generalist Embedding Models},
  year = 2024,
  eprint = {2405.17428},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL}
}

@misc{Choi2024LinqEmbed,
  author = {Chanyeol Choi and Junseong Kim and Seolhwa Lee and Jihoon Kwon and Sangmo Gu and Yejin Kim and Minkyung Cho and Jy-yong Sohn},
  title = {Linq-Embed-Mistral: Technical Report},
  year = 2024,
  eprint = {2412.03223},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL}
}

@misc{Meng2024SFREmbed,
  author = {Rui Meng and Ye Liu and Shafiq Rayhan Joty and Caiming Xiong and Yingbo Zhou and Semih Yavuz},
  title = {{SFR-Embedding-2}: Advanced Text Embedding with Multi-stage Training},
  year = 2024,
  howpublished = {\url{https://huggingface.co/Salesforce/SFR-Embedding-2_R}}
}

@misc{Zhang2024JasperStella,
  author = {Dun Zhang and Jiacheng Li and Ziyang Zeng and Fulong Wang},
  title = {Jasper and Stella: Distillation of {SOTA} Embedding Models},
  year = 2024,
  eprint = {2412.19048},
  archivePrefix = {arXiv},
  primaryClass = {cs.IR}
}

%%%%%%%%% Multimodal Evaluation Metrics %%%%%%%%%

@inproceedings{Shi2022EMScore,
  author = {Yaya Shi and Xu Yang and Haiyang Xu and Chunfeng Yuan and Bing Li and Weiming Hu and Zheng-Jun Zha},
  title = {{EMScore}: Evaluating Video Captioning via Coarse-Grained and Fine-Grained Embedding Matching},
  booktitle = CVPR,
  pages = {17929--17938},
  year = 2022
}

@inproceedings{Sarto2023PAC,
  author = {Sara Sarto and Manuele Barraco and Marcella Cornia and Lorenzo Baraldi and Rita Cucchiara},
  title = {Positive-Augmented Contrastive Learning for Image and Video Captioning Evaluation},
  booktitle = CVPR,
  pages = {6914--6924},
  year = 2023
}

@inproceedings{Radford2021CLIP,
  author = {Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  title = {Learning Transferable Visual Models From Natural Language Supervision},
  booktitle = {Proc. Int. Conf. Mach. Learn.},
  year = 2021
}

@inproceedings{Hessel2021CLIPScore,
  author = {Jack Hessel and Ari Holtzman and Maxwell Forbes and Ronan Le Bras and Yejin Choi},
  title = {{CLIPScore}: A Reference-free Evaluation Metric for Image Captioning},
  booktitle = {Proc. Conf. Empirical Methods Natural Lang. Process.},
  pages = {7514--7528},
  year = 2021
}

@misc{Wang2024Tarsier,
  author = {Jiawei Wang and Liping Yuan and Yuchen Zhang and Haomiao Sun},
  title = {Tarsier: Recipes for Training and Evaluating Large Video Description Models},
  year = 2024,
  eprint = {2407.00634},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV}
}

@inproceedings{Dubey2025KeyFrame,
  author = {Harsh Dubey and Chulwoo Pack},
  title = {Leveraging Textual Memory and Key Frame Reasoning for Full Video Understanding Using Off-the-Shelf {LLMs} and {VLMs}},
  booktitle = AAAI,
  pages = {12445--12446},
  year = 2025,
  note = {Student Abstract}
}

%%%%%%%%% LLM-Based Metrics %%%%%%%%%

@inproceedings{Chan2023CLAIR,
  author = {David M. Chan and Suzanne Petryk and Joseph E. Gonzalez and Trevor Darrell and John Canny},
  title = {{CLAIR}: Evaluating Image Captions with Large Language Models},
  booktitle = {Proc. Conf. Empirical Methods Natural Lang. Process.},
  pages = {13638--13646},
  year = 2023
}

@misc{Cheng2025CapArena,
  author = {Kanzhi Cheng and Wenpo Song and Jiaxin Fan and Zheng Ma and Qiushi Sun and Fangzhi Xu and Chenyang Yan and Nuo Chen and Jianbing Zhang and Jiajun Chen},
  title = {{CapArena}: Benchmarking and Analyzing Detailed Image Captioning in the {LLM} Era},
  year = 2025,
  eprint = {2503.12329},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV}
}

@misc{Li2024Wolf,
  author = {Boyi Li and Ligeng Zhu and Ran Tian and Shuhan Tan and Yuxiao Chen and Yao Lu and Yin Cui and Sushant Veer and Max Ehrlich and Jonah Philion and Xinshuo Weng and Fuzhao Xue and Jim Fan and Yuke Zhu and Jan Kautz and Andrew Tao and Ming-Yu Liu and Sanja Fidler and Boris Ivanovic and Trevor Darrell and Jitendra Malik and Song Han and Marco Pavone},
  title = {Wolf: Dense Video Captioning with a World Summarization Framework},
  year = 2024,
  eprint = {2407.18908},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV}
}
